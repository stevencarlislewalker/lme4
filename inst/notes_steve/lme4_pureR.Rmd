Intro to numerical approaches to mixed models: or simpler versions of lme4 in pure R
====================================================================================

Introduction
------------

The current development version of the `lme4` package is difficult for me to understand.  The complexity arises due to:

1. `C++` code
2. heavy use of reference classes, 
3. fill-reducing permutations for sparse Cholesky decompositions (which I gather are useful for speed, but difficult for me), 
4. and interactions between these things.

Here I decribe my work on writing a version in pure `R` that uses simpler linear algebra.

Assumptions
-----------

I think that all I assume is an understanding of three ideas:

1.  matrix multiplication (two matrices become one!)
2.  matrix decomposition (one matrix becomes two!)
3.  solving linear matrix equations
4.  that both of these things can be done on a computer, without having to think about the details of these computations

Basic numerical problem
-----------------------

One of the coolest things about linear mixed effects models as fitted by the `lmer` function, is that the fitting proceedure consists of iteratively solving a series of linear matrix equations.  A general such linear matrix equation posses the problem of finding a matrix $latex \mathbf W$, given,

$$latex \mathbf A \mathbf W = \mathbf C $$

where $latex \mathbf A $ is square and symmetric.  This can be done directly on a computer.  For example, in `R` we would use the `solve` command,

```{r}
set.seed(1)
(A <- matrix(rnorm(4), 2, 2))
(C <- matrix(rnorm(4), 2, 2))
(W <- solve(A, C))
```

This is fine if we only want to solve a few of these very small problems.  However, when fitting complex linear mixed effects models to very large data sets, this method becomes way to slow and so we need to take a less direct approach.  The approach that `lme4` takes is based on the Cholesky decomposition.

Cholesky decomposition
----------------------

Think of this decomposition as the analogue of square root for matrices.  Using this decomposition, we can solve $latex \mathbf A \mathbf W = \mathbf C $ in three steps (rather than the single step above).  This sounds worse at first:  three steps instead of a single step.  But it turns out that solving the following three problems is faster than the single problem above,

1. Find the Cholesky factor, $latex \mathbf L$ (i.e. matrix square root), of $latex \mathbf A$, $$latex \mathbf A = \mathbf L \mathbf L^\top$$
2. Solve the following equation for a matrix called $latex \mathbf T$, $$latex \mathbf L \mathbf T = \mathbf C $$
3. We can then solve for what we really want, $latex \mathbf W$, as, $$latex \mathbf W = \mathbf L^\top \mathbf W = \mathbf C $$

To understand the speed, consider this example,
```{r}
library(rbenchmark)
library(RcppEigen)
library(rmv)

set.seed(3)

n <- 100
A <- rcov(n+1, n)
C <- rnorm(n)

benchmark(
  explicit.chol = {
    L <- chol(A)
    T <- forwardsolve(t(L), C)
    backsolve(L, T)
  },
  solve = solve(A, C), 
  solve.inv = solve(A) %*% C,
  solve.with.Matrix = solve(Matrix(A), C), 
  qr.solve = qr.solve(A, C), 
  naive.explicit.chol = {
    L <- chol(A)
    T <- solve(t(L), C)
    solve(L, T)
  },
  solve.with.svd = with(svd(A), v %*% diag(1/d) %*% t(u) %*% C),
  replications = 100, order = 'relative', columns = c('test', 'relative'))
```


But what do these matrices mean?
--------------------------------

The trick is that each of these matrics have a complex structure underlying them.  Why do we need to worry about this structure?  Well, it is this structure that allows us to relate this simple matrtix equation to our statistical interpretation of it as a linear mixed model.

$$latex  \mathbf A = \begin{bmatrix}
\mathbf P\left(\Lambda_\theta^\top\mathbf Z^\top\mathbf
      Z\Lambda_\theta+\mathbf I_q\right)\mathbf P^\top &
    \mathbf P\Lambda_\theta^\top\mathbf Z^\top\mathbf X\\
    \mathbf X^\top\mathbf Z\Lambda_\theta\mathbf P^\top & \mathbf X^\top\mathbf X
  \end{bmatrix}
$$

$$latex  \mathbf w = 
  \begin{bmatrix}
    \mathbf P\tilde{\mathbf u}\\
    \widehat{\mathbf\beta}_\theta
  \end{bmatrix} $$
  
  $$latex  \mathbf c = 
  \begin{bmatrix}
    \mathbf P\Lambda_\theta^\top\mathbf Z^\top\mathbf y_{\text{obs}}\\
    \mathbf X^\top\mathbf y_{\text{obs}}
  \end{bmatrix} $$

To understand what these terms are I consider an example.  Although this example is meant to be as simple as possible, it is fairly lengthy and so a summary of the correspondence between the math symbols and the computational symbols follows.  I'm still going to use the formula parsing tools from `lme4` and I set the random number seed for replicability of the simulations below:

```{r}
library(lme4)
set.seed(1)
options(width = 100)
```

Simulating a **simple** null test data set:
```{r}
n <- 10 # sample size
p <- 2  # number of predictors

y <- rnorm(n)  # response
X <- matrix(rnorm(n*p), n, p)  # fixed effects design
f <- rep(c('a','b'), c(5, 5))  # grouping factor for random effects

(df <- cbind(y, as.data.frame(X), f))  # data frame with these data
````

Here is the model to be fitted to the data:
```{r}
fm <- y ~ V1 + V2 + (V1 + V2 | f)
```

```{r}
fm_df_glmer <- lme4:::glmer_form_parse(fm, df)
```

This `fm_df_glmer` object contains many objects required to fit the model to the data.  The first things are the design matrices---the transpose of the random effects design matrix, $latex \mathbf Z^\top$,
```{r}
(Zt <- fm_df_glmer$reTrms$Zt)
```
and the fixed effects design matrix, $latex \mathbf X$,
```{r}
(X <- fm_df_glmer$X)
```

There are several matrix products that both are used often in the fitting proceedure and are functions of the data only.  Therefore, we compute these upfront,
```{r}
XtX <- t(X) %*% X
Xty <- t(X) %*% y
ZtX <- Zt %*% X
Zty <- Zt %*% y
```

Another object in `gm_df_glmer` is `theta`, corresponding to $latex \theta$, which contains the initial values for the variance component parameters,
```{r}
(theta <- fm_df_glmer$reTrms$theta)
```
The meaning of these variance component parameter is somewhat difficult to understand.  Essentially, they fill in certain elements of the $latex \Lambda_\theta^\top$ matrix,
```{r}
(Lambdat <- fm_df_glmer$reTrms$Lambdat)
```
From a theoretical point of view this `Lambdat` matrix is a square root of the covariance matrix of the random effects.  From a computational point of view, this `Lambdat` matrix is a little different from other `R` matrices, in that it is a **sparse** matrix of class `dgCMatrix`. See documentation on the `Matrix` package for more information on this class.  The key piece of information for our purposes is that there is a slot that contains the non-zero values,
```{r}
Lambdat@x
```
The information on where to put these numbers in the matrix itself is stored in other slots called `i` and `p`.  However, we don't need to really worry about how this works.  We can calculate `Lambdat@x` manually by using another element of `rt`,
```{r}
(Lind <- fm_df_glmer$reTrms$Lind)
```
This vector contains the indices required to convert `theta` to `Lambdat@x`,
```{r}
theta[Lind]
```
Note that this result is identical to `Lambdat@x`.  This is not a coincidence.  This manual updating will come in handy during the model fitting proceedure.

I'll save the remaining objects in `fm_df_glmer$reTrms` but put off describing them for now (mostly because I don't understand them myself right now!),
```{r}
(Gp <- fm_df_glmer$reTrms$Gp)
(lower <- fm_df_glmer$reTrms$lower)
(flist <- fm_df_glmer$reTrms$flist)
(f <- fm_df_glmer$reTrms$cnms$f)
```

OK so that takes care of $latex \mathbf Z, \mathbf X$, and $latex \Lambda_\theta$.  Another simple one is $latex y_\mathrm{obs}$, which are the response data,

```{r}
y
```

The $latex \mathbf w $ vector contains the estimate (or value at the current iterate) of the random, $latex \tilde{\mathbf u}$, and fixed, $latex \widehat{\beta}_\theta$, effect coefficients.

I've left The strangest and most obscure aspect of the problem for last.  The matrix $latex \mathbf P$ is a permutation matrix that doesn't change the statistical interpretation of the estimates but does improve the computational speed of the fitting proceedure.  One of the more difficult aspects of this matrix for me is the fact that it is apparently never actually computed.  Instead, $latex \mathbf P$ is better thought of as a shorthand notation for using a particularly efficient method for solving a system of equations.

Later...
--------

For convenience, Doug Bates also defines another matrix,
```{r}
(Ut <- Lambdat %*% Zt)
```
And I find that a couple more matrices on this theme are also useful,
```{r}
UtU <- UtUpI <- Ut %*% t(Ut)
diag(UtUpI) <- diag(UtUpI) + 1
```
The Cholesky decomposition of `UtUpI` is used alot too,
```{r}
(L <- Cholesky(UtUpI))
```
The `L` object is a **sparse** Cholesky factor of class `dCHMsimpl`.

Here's something I don't get yet...updating.  Doug Bates uses lines like this,
```{r}
(L <- update(L, Ut, mult = 1))
```
In other words the `L` matrix is *updated*, but I'm perplexed because its no different than the original `L` in this case at least.  Maybe it'll make more sense later.















